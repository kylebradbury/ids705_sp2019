<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Syllabus | Principles of Machine Learning</title>

  <!-- bootstrap -->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap-theme.min.css">

  <!-- Google fonts -->
  <link href='https://fonts.googleapis.com/css?family=Roboto:700,400,300' rel='stylesheet' type='text/css'>
  <!-- <link href='http://fonts.googleapis.com/css?family=Lato:400,300' rel='stylesheet' type='text/css'> -->

  <!-- Google Analytics -->
  <script>
  </script>

  <link rel="stylesheet" type="text/css" href="style.css" />
</head>

<body>

<div id="header">

  <a href="https://kylebradbury.github.io/ids705/">
    <h1>IDS 705: Principles of Machine Learning</h1>
  </a>
  <div class='text-center'>
    <h4>Duke University</h4>
    <h4>Spring 2019</h4>
  </div>

  <div style="clear:both;"></div>
</div>


<div class="container sec">

  <h2>Schedule and Syllabus</h2>

  <br>
  The schedule below is a guide to what we will be covering throughout the semester and is subject to change to meet the learning goals of the class. Check this website regularly for the latest schedule and for course materials that will be posted here through links on the syllabus.<br>
  <i>ISL = <a href='http://www-bcf.usc.edu/~gareth/ISL/'>Introduction to Statistical Learning</a>, by James, Witten, Hastie, and Tibshirani</i><br>
  <i>DM = <a href='https://www-users.cs.umn.edu/~kumar001/dmbook/'>Introduction to Data Mining</a>, by Tan, Steinbach, Karpatne, and Kumar</i><br>
  <i>PRML = <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning</a>, by Bishop</i><br>
  <i>DL = <a href='https://www.deeplearningbook.org/'>Deep Learning</a>, by Goodfellow, Bengio, and Courville</i><br>
  <i>RL = <a href='http://incompleteideas.net/book/the-book.html'>Reinforcement Learning: An Introduction</a>, by Sutton and Barto</i><br>

</div>


<!-- <tr class="warning">    <tr class="danger">    <tr class="info"> -->

<div class="container sec">
<table class="table table-hover">
  <thead class="thead-light">
    <tr>
      <th>Event Type</th><th>Date</th><th>Description</th><th>Readings</th><th>Course Materials</th>
    </tr>
  </thead>
  <tr>
    <td>Lecture 1</td>
    <td>Wednesday<br> Jan 9</td>
    <td>
      <b>What is machine learning?</b> <br>
      Course overview and an orientation to the major branches of machine learning: unsupervised, supervised, and reinforcement learning 
    </td>
    <td>None</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture01_what_is_machine_learning.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 2</td>
    <td>Monday <br> Jan 14</td>
    <td>
      <b>An end-to-end machine learning example</b> <br>
      Stating the problem, creating the model, evaluating performance, and operationalizing the solution</td>
    <td>ISL Ch. 1 + 2.1</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture02_end-to-end_machine_learning.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 3</td>
    <td>Wednesday <br> Jan 16</td>
    <td>
      <b>How flexible should my algorithms be: the bias-variance tradeoff </b> <br>
      K-nearest neighbors classification and the bias-variance tradeoff 
    </td>
    <td>ISL 2.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture03_bias_variance_tradeoff.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="table-primary">
    <td>No class</td>
    <td>Monday<br> Jan 21</td>
    <td>Martin Luther King, Jr. Day</td>
    <td></td>
    <td></td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Jan 23</td>
    <td><b>Assignment #1 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment%201.ipynb">[assignment]</a></td>
  </tr>

  <tr>
    <td>Lecture 4</td>
    <td>Wednesday <br> Jan 23</td>
    <td>
      <b>Linear Models I</b> <br>
      Simple linear regression, multiple linear regression, measuring error, model fitting and least squares, comparing linear regression and classification
    </td>
    <td>ISL Intro of 3, 3.1, and 3.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture04_linear_models.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 5</td>
    <td>Monday <br> Jan 28</td>
    <td>
      <b>Linear Models II</b> <br>
      Nonlinear transformations of predictors, cost/loss functions, selecting parameters through gradient descent.
    </td>
    <td>ISL 3.3 and 3.5</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture05_linear_models_2.pdf'>[slides]</a>
    </td>
  </tr>
  
  <tr>
    <td>Lecture 6</td>
    <td>Wednesday <br> Jan 30</td>
    <td>
      <b>Performance evaluation and model comparison</b> <br>
      Choosing the right mode: accuracy vs speed vs interpretability; metrics for supervised learning performance evaluation: types of errors, receiver operating characteristics curves, confusion matrices
    </td>
    <td>ISL 4.1, 4.2, and 4.3</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture06_evaluating_performance_1.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 7</td>
    <td>Monday <br> Feb 4</td>
    <td>
      <b>Validation and model testing</b> <br>
      Resampling techniques: training, testing, and validation datasets, the importance of ensuring representative resampling, and cross validation
    </td>
    <td>ISL 5.1 and 5.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture07_evaluating_performance_2.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Feb 6</td>
    <td><b>Assignment #2 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment%202.ipynb">[assignment]</a></td>
  </tr>

  <tr>
    <td>Lecture 8</td>
    <td>Wednesday <br> Feb 6</td>
    <td>
      <b>Decision theory</b> <br>
      How to operate supervised learning algorithms in practice
    </td>
    <td><a href="http://canmedia.mheducation.ca/college/olcsupport/lind/5ce/Lind5ce_Ch17_An_Introduction_to_Decision_Theory.pdf">Link to reading</a></td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture08_decision_theory.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>  
    <td>Lecture 9</td>
    <td>Monday <br> Feb 11</td>
    <td>
      <b>Reducing overfit</b> <br>
      Model and feature selection; Occam’s razor; Subset selection; L1 (ridge), L2 (LASSO), and elastic net regularization.
    </td>
    <td>ISL 6.1 and 6.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture09_regularization.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 10</td>
    <td>Wednesday <br> Feb 13</td>
    <td>
      <b>Additional classification methods</b> <br>
      Linear discriminant analysis and naïve Bayes
    </td>
    <td>ISL 4.4 and 4.5</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture10_classification.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 11</td>
    <td>Monday <br> Feb 18</td>
    <td>
      <b>Tree-based models and ensembles</b> <br>
      From decision trees to random forests: bagging, bootstrapping, and boosting
    </td>
    <td>ISL 8.1 and 8.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture11_trees_and_ensembles.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td><strike>Wednesday<br> Feb 20</strike><br>Friday<br> Feb 22</td>
    <td><b>Assignment #3 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment%203.ipynb">[assignment]</a></td>
  </tr>



  <tr>
    <td>Lecture 12</td>
    <td>Wednesday <br> Feb 20</td>
    <td>
      <b>Dimensionality reduction</b> <br>
      The Curse of Dimensionality and intro to principal components analysis (PCA)
    </td>
    <td>ISL 6.3, 6.4, 10.1, and 10.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture12_dimensionality_reduction.pdf'>[slides]</a>
    </td>
  </tr>

  <tr>
    <td>Lecture 13</td>
    <td>Monday <br> Feb 25</td>
    <td>
      <b>Principal components analysis (PCA)</b> <br>
      Explaining how PCA works and how we calculate the principal components.
    </td>
    <td>ISL 10.3</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture12_dimensionality_reduction.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Feb 27</td>
    <td><b>End of Kaggle Competition</b></td>
    <td></td>
    <td><a href="kaggle.html">[kaggle competition]</a></td>
  </tr>

  <tr>
    <td>Lecture 14</td>
    <td>Wednesday <br> Feb 27</td>
    <td>
      <b>Clustering I</b> <br>
      From K-means to Gaussian mixture model clustering and Expectation Maximization
    </td>
    <td>DM Ch 7 (<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">link</a>): Intro, 7.1 and 7.2</td>
    <td>
      <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture14_clustering_1.pdf'>[slides]</a>
      <br>
    </td>
  </tr>

  <tr>
    <td>Lecture 15</td>
    <td>Monday <br> Mar 4</td>
    <td>
      <b>Clustering II</b> <br>
      Hierarchical clustering, DBSCAN, and spectral clustering
    </td>
   <td>DM Ch 7 (<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">link</a>): 7.3 and 7.4</td>
   <td>
     <a href='https://github.com/kylebradbury/ids705/raw/master/lectures/lecture15_clustering_2.pdf'>[slides]</a>
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Wednesday<br> Mar 6</td>
    <td><b>Kaggle Competition Reports Due</b></td>
    <td></td>
    <td><a href="kaggle.html#report">[kaggle report]</a></td>
  </tr>

  <tr>
    <td>Lecture 16</td>
    <td>Wednesday <br> Mar 6</td>
    <td>
      <b>Neural networks I</b> <br>
      How a neural network works
    </td>
    <td>PRML Ch 5: 5.1 </td>
    <td>
      [slides]
    </td>
  </tr>

    <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Friday<br> Mar 8</td>
    <td><b>Kaggle Team Peer Evaluation</b></td>
    <td></td>
    <td><a href="kaggle.html#peer_evaluation">[peer evaluation]</a></td>
  </tr>

  <tr class="table-primary">
    <td>No class</td>
    <td>Mar 11-15</td>
    <td>Spring break week</td>
    <td></td>
    <td></td>
  </tr>

  <tr>
    <td>Lecture 17</td>
    <td>Monday <br> Mar 18</td>
    <td>
      <b>Neural networks II</b> <br>
      Backpropagation
    </td>
    <td>PRML Ch 5: 5.3 (intro), 5.3.1, 5.3.2, and <a href="http://colah.github.io/posts/2015-08-Backprop/">Calculus on Computational Graphs</a></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 18</td>
    <td>Wednesday <br> Mar 20</td>
    <td>
      <b>Introduction to Deep learning</b> <br>
      Implementing deep learning models in Keras
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Monday <br> Mar 25</td>
    <td><b>Assignment #4 Due</b></td>
    <td></td>
    <td><a href="https://github.com/kylebradbury/ids705/blob/master/assignments/Assignment%204.ipynb">[assignment]</a></td>
  </tr>

  <tr>
    <td>Lecture 19</td>
    <td>Monday <br> Mar 25</td>
    <td>
      <b>Reinforcement Learning I</b> <br>
      Formulating the reinforcement learning problem
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 20</td>
    <td>Wednesday <br> Mar 27</td>
    <td>
      <b>Reinforcement Learning II</b> <br>
      Policy and value functions, rewards, and introduction to Markov processes 
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Monday <br> Apr 1</td>
    <td><b>Final Project Proposal</b></td>
    <td></td>
    <td>[final project]</td>
  </tr>

  <tr>
    <td>Lecture 21</td>
    <td>Monday <br> Apr 1</td>
    <td>
      <b>Reinforcement Learning III</b> <br>
      From Markov Chains to Markov Decision Processes (MDPs)
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 22</td>
    <td>Wednesday <br> Apr 3</td>
    <td>
      <b>Reinforcement Learning IV</b> <br>
      Finding optimal policies through policy iteration, value iteration, and Monte Carlo methods
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Monday <br> Apr 8</td>
    <td><b>Assignment #5 Due</b></td>
    <td></td>
    <td>
      [assignment]
    </td>
  </tr>

  <tr>
    <td>Lecture 23</td>
    <td>Monday <br> Apr 8</td>
    <td>
      <b>Kernel Smoothing</b> <br>
      Non-parametric methods including kernel density estimation, kernel regression, and local regression
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 24</td>
    <td>Wednesday <br> Apr 10</td>
    <td>
      <b>Kernel Methods</b> <br>
      Introducing Kernel machines via the kernel perceptron, maximum margin classifiers, and support vector machines
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr>
    <td>Lecture 25</td>
    <td>Monday <br> Apr 15</td>
    <td>
      <b>State-of-the-art machine learning and applications</b> <br>
      Cutting-edge applications and techniques: ideas on where the field is heading and how to stay up-to-date
    </td>
    <td></td>
    <td>
      [slides]
    </td>
  </tr>

  <tr class="table-danger">
    <td><b>Deliverable</b></td>
    <td>Wednesday <br> Apr 17</td>
    <td>
      <b>Final project video showcase and competition (last class meeting of the semester)</b> <br>
      The grand finale of the semester in which all of the videos you produced will be shown to the class
    </td>
    <td></td>
    <td>
      [final project]
    </td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Thursday<br> Apr 18</td>
    <td><b>Final Report</b></td>
    <td></td>
    <td>[final project]</td>
  </tr>

  <tr class="table-warning">
    <td><b>Deliverable</b></td>
    <td>Friday<br> Apr 19</td>
    <td><b>Final Project Peer Evaluation</b></td>
    <td></td>
    <td>[peer evaluation]</td>
  </tr>

</table>
</div>

<div class="sechighlight">
<div id="footer">
  <div id="footermsg">Website design adapted from the <a href="http://cs231n.stanford.edu/">Stanford CS231 course page</a></div>
</div>
</div>

<!-- jQuery and Boostrap -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
</body>

</html>
